{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses Milvus through Docker Compose so you must have Docker installed to run this notebook (Milvus is spun up via `docker compose up -d` as shown in the block below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -qU pymilvus langchain sentence-transformers tiktoken octoai-sdk openai\n",
    "# docker-compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OCTOAI_TOKEN\"] = os.getenv(\"OCTOAI_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sillygoose/dev/rag_cookbooks/lib/python3.12/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! model is not default parameter.\n",
      "                model was transferred to model_kwargs.\n",
      "                Please confirm that model is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.octoai_endpoint import OctoAIEndpoint\n",
    "llm = OctoAIEndpoint(\n",
    "        model=\"mixtral-8x22b-instruct-fp16\",\n",
    "        max_tokens=50000,\n",
    "        presence_penalty=0,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OctoAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OctoAIEmbeddings(endpoint_url=\"https://text.octoai.run/v1/embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "files = []\n",
    "\n",
    "def download_pdf(url, save_path): \n",
    "    paths = []\n",
    "    response = requests.get(url)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        paths.append(save_path)\n",
    "        f.write(response.content)\n",
    "\n",
    "pdf_url = \"https://pages.cs.wisc.edu/~remzi/OSTEP/cpu-intro.pdf\" \n",
    "save_path = \"cpu-intro.pdf\"\n",
    "download_pdf(pdf_url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk too large, further splitting: 22151 characters\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cpu-mechanisms.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m all_documents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m---> 53\u001b[0m     all_documents\u001b[38;5;241m.\u001b[39mextend(\u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Ensure no document exceeds the length limit\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m all_documents:\n",
      "Cell \u001b[0;32mIn[95], line 24\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_file\u001b[39m(file_path):\n\u001b[1;32m     23\u001b[0m     file_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpdfplumber\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pdf:\n\u001b[1;32m     25\u001b[0m         all_text \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf\u001b[38;5;241m.\u001b[39mpages:\n",
      "File \u001b[0;32m~/dev/rag_cookbooks/lib/python3.12/site-packages/pdfplumber/pdf.py:86\u001b[0m, in \u001b[0;36mPDF.open\u001b[0;34m(cls, path_or_fp, pages, laparams, password, strict_metadata, repair, gs_path)\u001b[0m\n\u001b[1;32m     84\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_fp, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath)):\n\u001b[0;32m---> 86\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_or_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     stream_is_external \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(path_or_fp)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cpu-mechanisms.pdf'"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings import OctoAIEmbeddings\n",
    "from langchain_community.vectorstores import Milvus\n",
    "\n",
    "files = [\"cpu-intro.pdf\"]\n",
    "# Initialize the embeddings API\n",
    "embeddings = OctoAIEmbeddings(endpoint_url=\"https://text.octoai.run/v1/embeddings\")\n",
    "\n",
    "def split_large_chunks(text, max_size=1000):\n",
    "    \"\"\"Split large chunks into smaller parts that comply with the size limit.\"\"\"\n",
    "    parts = []\n",
    "    while len(text) > max_size:\n",
    "        part = text[:max_size]\n",
    "        parts.append(part)\n",
    "        text = text[max_size:]\n",
    "    if text:\n",
    "        parts.append(text) \n",
    "    return parts\n",
    "\n",
    "def process_file(file_path):\n",
    "    file_texts = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        all_text = []\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()  \n",
    "            if page_text:\n",
    "                all_text.append(page_text)\n",
    "\n",
    "        combined_text = \"\\n\".join(all_text)\n",
    "        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=8000, chunk_overlap=240\n",
    "        )\n",
    "        texts = text_splitter.split_text(combined_text)\n",
    "        for i, chunked_text in enumerate(texts):\n",
    "            if len(chunked_text) > 10240:\n",
    "                print(f\"Chunk too large, further splitting: {len(chunked_text)} characters\")\n",
    "                smaller_chunks = split_large_chunks(chunked_text)\n",
    "                for j, small_chunk in enumerate(smaller_chunks):\n",
    "                    if len(small_chunk) > 10240:\n",
    "                        print(f\"Error: Sub-chunk still too large: {len(small_chunk)} characters\")\n",
    "                        continue\n",
    "                    file_texts.append(Document(page_content=small_chunk,\n",
    "                        metadata={\"doc_title\": file_path.split(\".\")[0], \"chunk_num\": f\"{i}-{j}\"}))\n",
    "            else:\n",
    "                file_texts.append(Document(page_content=chunked_text,\n",
    "                        metadata={\"doc_title\": file_path.split(\".\")[0], \"chunk_num\": i}))\n",
    "    return file_texts\n",
    "\n",
    "all_documents = []\n",
    "for file in files:\n",
    "    all_documents.extend(process_file(file))\n",
    "\n",
    "try:\n",
    "    # Ensure no document exceeds the length limit\n",
    "    for doc in all_documents:\n",
    "        if len(doc.page_content) > 10240:\n",
    "            raise ValueError(f\"Document exceeds max length: {len(doc.page_content)} characters\")\n",
    "    vector_store = Milvus.from_documents(\n",
    "        all_documents,\n",
    "        embedding=embeddings,\n",
    "        connection_args={\"host\": \"localhost\", \"port\": 19530},\n",
    "        collection_name=\"motion100\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Failed to create embeddings due to an error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def quiz_loop(questions):\n",
    "    score = 0\n",
    "    missed_topics = {}\n",
    "\n",
    "    for key, question in questions.items():\n",
    "        print(f\"Question: {question}\")\n",
    "        answer = input(\"Enter your answer (or type 'skip' to pass): \")\n",
    "        if answer.lower() == 'skip':\n",
    "            missed_topics[key] = question\n",
    "            continue\n",
    "\n",
    "        correctness = chain.invoke(f\"score the response based on the accuracy of the input. output a python bool if answer: {answer} is a correct answer for the question: {question} output only 'True' if true, output only 'False' if false\")\n",
    "        print(correctness)\n",
    "        if correctness == \"True\":\n",
    "            score += 1\n",
    "        else:\n",
    "            missed_topics[key] = question\n",
    "\n",
    "    return score, missed_topics\n",
    "\n",
    "\n",
    "missedTopics = \"\"\n",
    "\n",
    "while True:\n",
    "    prompt = \"Based on the content of the provided chapter, generate a list of two detailed questions that cover key concepts and themes. These concepts and themes must be returned in json with a key: value format. after the two responses are provided, you can ask me two more questions and continue the process. \"\n",
    "    questions_json = chain.invoke(prompt)\n",
    "    questions = json.loads(questions_json)\n",
    "\n",
    "    print(\"\\nStarting the quiz...\")\n",
    "    score, missed_topics = quiz_loop(questions)\n",
    "    print(f\"Initial score: {score}/{len(questions)}\")\n",
    "    \n",
    "    if missed_topics:\n",
    "        print(\"\\nYou missed some questions. Let's try those again.\")\n",
    "        score, _ = quiz_loop(missed_topics)\n",
    "        print(f\"Re-quiz score: {score}/{len(missed_topics)}\")\n",
    "\n",
    "    continue_quiz = input(\"Do you want to continue studying? (yes/no): \").lower()\n",
    "    if continue_quiz != 'yes':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
