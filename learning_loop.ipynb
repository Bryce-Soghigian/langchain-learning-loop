{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses Milvus through Docker Compose so you must have Docker installed to run this notebook (Milvus is spun up via `docker compose up -d` as shown in the block below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -qU pymilvus langchain sentence-transformers tiktoken octoai-sdk openai\n",
    "# docker-compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OCTOAI_TOKEN\"] = os.getenv(\"OCTOAI_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sillygoose/dev/rag_cookbooks/lib/python3.12/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! model is not default parameter.\n",
      "                model was transferred to model_kwargs.\n",
      "                Please confirm that model is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.octoai_endpoint import OctoAIEndpoint\n",
    "llm = OctoAIEndpoint(\n",
    "        model=\"mixtral-8x22b-instruct-fp16\",\n",
    "        max_tokens=50000,\n",
    "        presence_penalty=0,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OctoAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OctoAIEmbeddings(endpoint_url=\"https://text.octoai.run/v1/embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "files = []\n",
    "\n",
    "def download_pdf(url, save_path): \n",
    "    paths = []\n",
    "    response = requests.get(url)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        paths.append(save_path)\n",
    "        f.write(response.content)\n",
    "\n",
    "pdf_url = \"https://pages.cs.wisc.edu/~remzi/OSTEP/cpu-intro.pdf\" \n",
    "save_path = \"cpu-intro.pdf\"\n",
    "files = download_pdf(pdf_url, save_path)\n",
    "pdf_url2 = \"https://pages.cs.wisc.edu/~remzi/OSTEP/cpu-api.pdf\"\n",
    "save_path2 = \"cpu-api\"\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk too large, further splitting: 15377 characters\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings import OctoAIEmbeddings\n",
    "from langchain_community.vectorstores import Milvus\n",
    "\n",
    "# Initialize the embeddings API\n",
    "embeddings = OctoAIEmbeddings(endpoint_url=\"https://text.octoai.run/v1/embeddings\")\n",
    "\n",
    "def split_large_chunks(text, max_size=1000):\n",
    "    \"\"\"Split large chunks into smaller parts that comply with the size limit.\"\"\"\n",
    "    parts = []\n",
    "    while len(text) > max_size:\n",
    "        part = text[:max_size]\n",
    "        parts.append(part)\n",
    "        text = text[max_size:]\n",
    "    if text:\n",
    "        parts.append(text) \n",
    "    return parts\n",
    "\n",
    "def process_file(file_path):\n",
    "    file_texts = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        all_text = []\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()  \n",
    "            if page_text:\n",
    "                all_text.append(page_text)\n",
    "\n",
    "        combined_text = \"\\n\".join(all_text)\n",
    "        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=8000, chunk_overlap=240\n",
    "        )\n",
    "        texts = text_splitter.split_text(combined_text)\n",
    "        for i, chunked_text in enumerate(texts):\n",
    "            if len(chunked_text) > 10240:\n",
    "                print(f\"Chunk too large, further splitting: {len(chunked_text)} characters\")\n",
    "                smaller_chunks = split_large_chunks(chunked_text)\n",
    "                for j, small_chunk in enumerate(smaller_chunks):\n",
    "                    if len(small_chunk) > 10240:\n",
    "                        print(f\"Error: Sub-chunk still too large: {len(small_chunk)} characters\")\n",
    "                        continue\n",
    "                    file_texts.append(Document(page_content=small_chunk,\n",
    "                        metadata={\"doc_title\": file_path.split(\".\")[0], \"chunk_num\": f\"{i}-{j}\"}))\n",
    "            else:\n",
    "                file_texts.append(Document(page_content=chunked_text,\n",
    "                        metadata={\"doc_title\": file_path.split(\".\")[0], \"chunk_num\": i}))\n",
    "    return file_texts\n",
    "\n",
    "files = [\"cpu-intro.pdf\", \"cpu-api.pdf\"]\n",
    "all_documents = []\n",
    "for file in files:\n",
    "    all_documents.extend(process_file(file))\n",
    "\n",
    "try:\n",
    "    # Ensure no document exceeds the length limit\n",
    "    for doc in all_documents:\n",
    "        if len(doc.page_content) > 10240:\n",
    "            raise ValueError(f\"Document exceeds max length: {len(doc.page_content)} characters\")\n",
    "    vector_store = Milvus.from_documents(\n",
    "        all_documents,\n",
    "        embedding=embeddings,\n",
    "        connection_args={\"host\": \"localhost\", \"port\": 19530},\n",
    "        collection_name=\"motion100\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Failed to create embeddings due to an error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting the quiz...\n",
      "Question: What is the definition of force according to Newton's First Law and how does it relate to the acceleration of a body?\n",
      " False\n",
      "Question: How do forces add as vectors according to Newton's Second Law, and what is the significance of this in understanding the motion of a body?\n",
      " True\n",
      "Initial score: 0/2\n",
      "\n",
      "You missed some questions. Let's try those again.\n",
      "Question: What is the definition of force according to Newton's First Law and how does it relate to the acceleration of a body?\n",
      " True\n",
      "Question: How do forces add as vectors according to Newton's Second Law, and what is the significance of this in understanding the motion of a body?\n",
      " True\n",
      "Re-quiz score: 0/2\n",
      "\n",
      "Starting the quiz...\n",
      "Question: What is the definition of force according to Newton's First Law and how does it relate to the acceleration of a body?\n",
      " False\n",
      "Question: How do forces act on two bodies according to Newton's Third Law, and what is the significance of this law in understanding interactions between bodies?\n",
      " True\n",
      "Initial score: 0/2\n",
      "\n",
      "You missed some questions. Let's try those again.\n",
      "Question: What is the definition of force according to Newton's First Law and how does it relate to the acceleration of a body?\n",
      " True\n",
      "Question: How do forces act on two bodies according to Newton's Third Law, and what is the significance of this law in understanding interactions between bodies?\n",
      " True\n",
      "Re-quiz score: 0/2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def quiz_loop(questions):\n",
    "    score = 0\n",
    "    missed_topics = {}\n",
    "\n",
    "    for key, question in questions.items():\n",
    "        print(f\"Question: {question}\")\n",
    "        answer = input(\"Enter your answer (or type 'skip' to pass): \")\n",
    "        if answer.lower() == 'skip':\n",
    "            missed_topics[key] = question\n",
    "            continue\n",
    "\n",
    "        correctness = chain.invoke(f\"score the response based on the accuracy of the input. output a python bool if answer: {answer} is a correct answer for the question: {question} output only 'True' if true, output only 'False' if false\")\n",
    "        print(correctness)\n",
    "        if correctness == \"True\":\n",
    "            score += 1\n",
    "        else:\n",
    "            missed_topics[key] = question\n",
    "\n",
    "    return score, missed_topics\n",
    "\n",
    "\n",
    "missedTopics = \"\"\n",
    "\n",
    "while True:\n",
    "    prompt = \"Based on the content of the provided chapter, generate a list of two detailed questions that cover key concepts and themes. These concepts and themes must be returned in json with a key: value format. after the two responses are provided, you can ask me two more questions and continue the process. \"\n",
    "    questions_json = chain.invoke(prompt)\n",
    "    questions = json.loads(questions_json)\n",
    "\n",
    "    print(\"\\nStarting the quiz...\")\n",
    "    score, missed_topics = quiz_loop(questions)\n",
    "    print(f\"Initial score: {score}/{len(questions)}\")\n",
    "    \n",
    "    if missed_topics:\n",
    "        print(\"\\nYou missed some questions. Let's try those again.\")\n",
    "        score, _ = quiz_loop(missed_topics)\n",
    "        print(f\"Re-quiz score: {score}/{len(missed_topics)}\")\n",
    "\n",
    "    continue_quiz = input(\"Do you want to continue studying? (yes/no): \").lower()\n",
    "    if continue_quiz != 'yes':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
